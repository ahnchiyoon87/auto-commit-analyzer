#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, textwrap, datetime as dt
from typing import List, Optional, Dict, Any
from dateutil import tz
from tenacity import retry, wait_random_exponential, stop_after_attempt
from github import Github
from openai import OpenAI, BadRequestError
from dotenv import load_dotenv

# ======================================
# ğŸ”§ 1. í™˜ê²½ì„¤ì •
# ======================================
load_dotenv()

# --- [A] í•˜ë“œì½”ë”© ì„¤ì • (ì›í•˜ë©´ ì´ ë¸”ë¡ë§Œ ìˆ˜ì •) ---
DEFAULT_REPOS = [
    "msa-ez/legacy-modernizer-frontend",
    "uengine-oss/legacy-modernizer-backend",
    "ahnchiyoon87/Antlr-Server"
]
MY_GITHUB_LOGIN = os.getenv("MY_GITHUB_LOGIN", "your-github-id")     # GitHub ë¡œê·¸ì¸ ì•„ì´ë””
MY_GITHUB_EMAIL = os.getenv("MY_GITHUB_EMAIL", "you@company.com")     # ì»¤ë°‹ ì´ë©”ì¼
OPENAI_MODEL    = os.getenv("OPENAI_MODEL", "gpt-4o-mini")            # OpenAI ëª¨ë¸
BRANCH          = os.getenv("BRANCH", "main")                         # ë¶„ì„í•  ë¸Œëœì¹˜
OUT_DIR         = os.getenv("OUT_DIR", "./reports")                   # ê²°ê³¼ ì €ì¥ í´ë”
os.makedirs(OUT_DIR, exist_ok=True)

# ëŒ€ìš©ëŸ‰ ì»¤ë°‹ ìš”ì•½ ì‹œ íŒŒì¼ì„ ë‚˜ëˆ„ëŠ” ë‹¨ìœ„(ê¶Œì¥: 8~15)
MAX_FILES_PER_CALL = int(os.getenv("MAX_FILES_PER_CALL", "6"))

# --- [B] í† í° ê´€ë¦¬ ---
GITHUB_TOKEN   = os.getenv("GITHUB_TOKEN", "ghp_...")                 # GitHub Personal Access Token
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-...")                # OpenAI API Key

if not GITHUB_TOKEN or not OPENAI_API_KEY:
    raise RuntimeError("âŒ GITHUB_TOKEN ë˜ëŠ” OPENAI_API_KEYê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. .envì— ì„¤ì •í•˜ì„¸ìš”.")

# ======================================
# ğŸ“¦ 2. ë°ì´í„° ëª¨ë¸ ì •ì˜ (Pydantic ì—†ì´)
# ======================================
class FileFinding:
    def __init__(self, file_path: str, change_type: str, summary: str, risk_level: str,
                 breaking_changes: List[str] = None, test_impact: List[str] = None,
                 migration_notes: List[str] = None, owner_guess: Optional[str] = None):
        self.file_path = file_path
        self.change_type = change_type
        self.summary = summary
        self.risk_level = risk_level
        self.breaking_changes = breaking_changes or []
        self.test_impact = test_impact or []
        self.migration_notes = migration_notes or []
        self.owner_guess = owner_guess

    def to_dict(self) -> Dict[str, Any]:
        return {
            "file_path": self.file_path,
            "change_type": self.change_type,
            "summary": self.summary,
            "risk_level": self.risk_level,
            "breaking_changes": self.breaking_changes,
            "test_impact": self.test_impact,
            "migration_notes": self.migration_notes,
            "owner_guess": self.owner_guess
        }

class CommitFinding:
    def __init__(self, repo: str, sha: str, author: Optional[str], author_login: Optional[str],
                 author_email: Optional[str], date_kst: str, title: str, files: List[FileFinding],
                 overall_summary: str, overall_risk: str):
        self.repo = repo
        self.sha = sha
        self.author = author
        self.author_login = author_login
        self.author_email = author_email
        self.date_kst = date_kst
        self.title = title
        self.files = files
        self.overall_summary = overall_summary
        self.overall_risk = overall_risk

class ReportModel:
    def __init__(self, generated_at: str, model: str, note_date_kst: str, repos: List[str],
                 branch: str, author_filter: dict, commits: List[CommitFinding]):
        self.generated_at = generated_at
        self.model = model
        self.note_date_kst = note_date_kst
        self.repos = repos
        self.branch = branch
        self.author_filter = author_filter
        self.commits = commits

    def to_dict(self) -> Dict[str, Any]:
        return {
            "generated_at": self.generated_at,
            "model": self.model,
            "note_date_kst": self.note_date_kst,
            "repos": self.repos,
            "branch": self.branch,
            "author_filter": self.author_filter,
            "commits": [
                {
                    "repo": c.repo,
                    "sha": c.sha,
                    "author": c.author,
                    "author_login": c.author_login,
                    "author_email": c.author_email,
                    "date_kst": c.date_kst,
                    "title": c.title,
                    "files": [f.to_dict() for f in c.files],
                    "overall_summary": c.overall_summary,
                    "overall_risk": c.overall_risk
                } for c in self.commits
            ]
        }

# ======================================
# â° 3. ë‚ ì§œ ê³„ì‚° (ì˜¤ëŠ˜ ë‚ ì§œ ìë™)
# ======================================
def get_today_kst_bounds():
    kst = tz.gettz("Asia/Seoul")
    now_kst = dt.datetime.now(tz=kst)
    note_date = now_kst.strftime("%Y-%m-%d")
    start_kst = now_kst.replace(hour=0, minute=0, second=0, microsecond=0)
    end_kst = start_kst + dt.timedelta(days=1)
    return note_date, start_kst.astimezone(tz.UTC), end_kst.astimezone(tz.UTC)

# ======================================
# ğŸ¤– 4. OpenAI & GitHub í´ë¼ì´ì–¸íŠ¸
# ======================================
gh = Github(GITHUB_TOKEN, per_page=100)
oai = OpenAI(api_key=OPENAI_API_KEY)

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì½”ë“œ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤. íŒŒì¼ ë³€ê²½ì‚¬í•­ì„ ë¶„ì„í•˜ê³  ê°„ë‹¨í•œ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”. "
    "JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ë˜: file_path, change_type, summary (ìµœëŒ€ 30ë‹¨ì–´), risk_level (low/medium/high). "
    "í•œê¸€ë¡œ ê°„ê²°í•˜ê²Œ í•µì‹¬ ë³€ê²½ì‚¬í•­ë§Œ ì„¤ëª…í•˜ì„¸ìš”."
)

# ======================================
# ğŸ§  5. LLM í˜¸ì¶œ
# ======================================
@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(4))
def analyze_file(file_path: str, change_type: str, patch: str) -> FileFinding:
    """íŒŒì¼ ë‹¨ìœ„ ìš”ì•½ (ì„±ê³µì ìœ¼ë¡œ ë™ì‘ ì¤‘)"""
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        response_format={"type":"json_object"},
        messages=[
            {"role":"system", "content": SYSTEM_PROMPT},
            {"role":"user", "content": f"file_path: {file_path}\nchange_type: {change_type}\n\nDIFF:\n{patch or ''}"}
        ]
    ).choices[0].message.content
    try:
        data = json.loads(resp)
        return FileFinding(
            file_path=data.get("file_path", file_path),
            change_type=data.get("change_type", change_type),
            summary=data.get("summary", "LLM parsing failed"),
            risk_level=data.get("risk_level", "medium"),
            breaking_changes=data.get("breaking_changes", []),
            test_impact=data.get("test_impact", []),
            migration_notes=data.get("migration_notes", []),
            owner_guess=data.get("owner_guess")
        )
    except Exception:
        return FileFinding(file_path=file_path, change_type=change_type, summary="LLM parsing failed", risk_level="medium")

def _lighten_files(files: List[FileFinding]) -> List[Dict[str, Any]]:
    """ì»¤ë°‹ ìš”ì•½ ì…ë ¥ì„ ìŠ¬ë¦¼í™” (í•„ìš” í•„ë“œë§Œ í¬í•¨)"""
    return [
        {
            "file_path": f.file_path,
            "change_type": f.change_type,
            "summary": f.summary,
            "risk_level": f.risk_level
        }
        for f in files
    ]

@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(4))
def _summarize_chunk(meta: dict, light_files_chunk: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ì²­í¬ ë‹¨ìœ„ ë¶€ë¶„ ìš”ì•½"""
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        response_format={"type":"json_object"},
        messages=[
            {"role":"system","content":"ì•„ë˜ íŒŒì¼ ë³€ê²½ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì»¤ë°‹ì˜ ì˜ë„/ì˜í–¥ì„ 5ì¤„ ë‚´ë¡œ JSONìœ¼ë¡œ ë°˜í™˜(overall_summary, overall_risk: low/medium/high)."},
            {"role":"user","content": json.dumps({"meta": {
                "repo": meta.get("repo",""),
                "sha": meta.get("sha",""),
                "title": meta.get("title","")
            }, "files": light_files_chunk}, ensure_ascii=False)}
        ]
    )
    out = resp.choices[0].message.content
    try:
        return json.loads(out)
    except Exception:
        return {"overall_summary":"(ë¶€ë¶„ ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨)","overall_risk":"medium"}

@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(4))
def _summarize_merge(partials: List[str]) -> Dict[str, Any]:
    """ë¶€ë¶„ ìš”ì•½ë“¤ì„ ìµœì¢… í†µí•©"""
    prompt = "\n\n".join(partials)
    resp = oai.chat.completions.create(
        model=OPENAI_MODEL,
        response_format={"type":"json_object"},
        messages=[
            {"role":"system","content":"ë¶€ë¶„ ìš”ì•½ë“¤ì„ í†µí•©í•´ ìµœì¢… overall_summary(6~10ë¬¸ì¥)ì™€ overall_risk(low/medium/high)ë§Œ JSONìœ¼ë¡œ ë°˜í™˜."},
            {"role":"user","content": prompt}
        ]
    )
    out = resp.choices[0].message.content
    try:
        return json.loads(out)
    except Exception:
        return {"overall_summary":"(ìµœì¢… í†µí•© ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨)","overall_risk":"medium"}

def fallback_summarize_commit(files: List[FileFinding], meta: dict) -> CommitFinding:
    """ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´"""
    order = {"high":0,"medium":1,"low":2}
    top = sorted(files, key=lambda x: order.get(x.risk_level,1))[:3]
    bullet = "\n".join([f"- {f.file_path} ({f.change_type}, {f.risk_level}): {f.summary}" for f in top])
    overall = (
        f"[ë¡œì»¬ìš”ì•½] {meta.get('title','')}\n"
        f"ì£¼ìš” ë³€ê²½ íŒŒì¼(ìƒìœ„ 3ê°œ):\n{bullet}"
    )
    return CommitFinding(
        repo=meta["repo"], sha=meta["sha"],
        author=meta.get("author"), author_login=meta.get("author_login"),
        author_email=meta.get("author_email"), date_kst=meta["date_kst"],
        title=meta["title"], files=files,
        overall_summary=overall,
        overall_risk=("high" if any(f.risk_level=="high" for f in files) else "medium")
    )

def summarize_commit(files: List[FileFinding], meta: dict) -> CommitFinding:
    """
    ì»¤ë°‹ ìš”ì•½:
    - ì…ë ¥ ìŠ¬ë¦¼í™”
    - íŒŒì¼ ë§ì„ ê²½ìš° ì²­í¬ ìš”ì•½ â†’ ìµœì¢… í†µí•©
    - ì‹¤íŒ¨ ì‹œ BadRequest ë©”ì‹œì§€ ì¶œë ¥ + ë¡œì»¬ Fallback
    """
    light_files = _lighten_files(files)
    try:
        # íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ 1íšŒ í˜¸ì¶œë¡œ ëë‚´ê¸°
        if len(light_files) <= MAX_FILES_PER_CALL:
            partial = _summarize_chunk(meta, light_files)
            final = partial
        else:
            # ì²­í¬ ìš”ì•½
            chunks = [light_files[i:i+MAX_FILES_PER_CALL] for i in range(0, len(light_files), MAX_FILES_PER_CALL)]
            partial_summaries = []
            for idx, ch in enumerate(chunks, 1):
                partial = _summarize_chunk(meta, ch)
                partial_summaries.append(partial.get("overall_summary",""))
            final = _summarize_merge(partial_summaries)

        return CommitFinding(
            repo=meta["repo"],
            sha=meta["sha"],
            author=meta.get("author"),
            author_login=meta.get("author_login"),
            author_email=meta.get("author_email"),
            date_kst=meta["date_kst"],
            title=meta["title"],
            files=files,
            overall_summary=final.get("overall_summary",""),
            overall_risk=final.get("overall_risk","medium")
        )
    except BadRequestError as e:
        # ìƒì„¸ ë©”ì‹œì§€ ìµœëŒ€í•œ í‘œì‹œ
        msg = getattr(e, "message", str(e))
        print(f"[COMMIT] âŒ BadRequest while summarizing {meta['sha'][:7]}: {msg}")
        return fallback_summarize_commit(files, meta)
    except Exception as e:
        print(f"[COMMIT] âŒ Unexpected error while summarizing {meta['sha'][:7]}: {e}")
        return fallback_summarize_commit(files, meta)

# ======================================
# ğŸ” 6. GitHub ì»¤ë°‹ í•„í„°
# ======================================
def commit_is_mine(commit) -> bool:
    login = getattr(commit.author, "login", None)
    email = getattr(commit.commit.author, "email", None)
    return (
        (login and login.strip().lower() == MY_GITHUB_LOGIN.strip().lower()) or
        (email and email.strip().lower() == MY_GITHUB_EMAIL.strip().lower())
    )

# ======================================
# ğŸ§¾ 7. ê²°ê³¼ ì¶œë ¥
# ======================================
def render_md(report: ReportModel) -> str:
    lines = [
        f"# ğŸ§  ì—°êµ¬ë…¸íŠ¸ â€” {report.note_date_kst} (KST)",
        "",
        f"- ìƒì„±ì‹œê°(UTC): {report.generated_at}",
        f"- ëª¨ë¸: `{report.model}`",
        f"- ë¦¬í¬ì§€í† ë¦¬: {', '.join(report.repos)}",
        f"- ë¸Œëœì¹˜: {report.branch}",
        f"- ì‘ì„±ì: {report.author_filter}",
        ""
    ]
    if not report.commits:
        lines.append("> ì˜¤ëŠ˜ì€ ë³¸ì¸ ì»¤ë°‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return "\n".join(lines)

    current_repo = None
    for c in report.commits:
        if c.repo != current_repo:
            current_repo = c.repo
            lines.append(f"## ğŸ“¦ {current_repo}\n")
        lines.append(f"### ğŸ”– {c.sha[:7]} â€” {c.title}")
        lines.append(f"- Date: {c.date_kst}")
        lines.append(f"- Risk: **{c.overall_risk}**")
        lines.append(f"- Repository: {c.repo}")
        lines.append(f"- Commit Link: https://github.com/{c.repo}/commit/{c.sha}")
        lines.append("")
        lines.append("> " + (c.overall_summary or "(ìš”ì•½ ì—†ìŒ)").replace("\n", "\n> "))
        lines.append("")
        lines.append("**ğŸ“ ë³€ê²½ëœ íŒŒì¼ë“¤:**")
        for f in c.files:
            lines.append(f"- `{f.file_path}` ({f.change_type}, risk={f.risk_level})")
            lines.append(f"  - {f.summary}")
        lines.append("")
    return "\n".join(lines)

# ======================================
# ğŸš€ 8. ë©”ì¸ ì‹¤í–‰
# ======================================
def main():
    note_date, since_utc, until_utc = get_today_kst_bounds()
    print(f"[INFO] Analyzing commits for {note_date} (KST)...")
    print(f"[INFO] Time range: {since_utc} ~ {until_utc} (UTC)")
    print(f"[INFO] Target repositories: {DEFAULT_REPOS}")
    print(f"[INFO] Branch: {BRANCH}")
    print(f"[INFO] Author filter: {MY_GITHUB_LOGIN} / {MY_GITHUB_EMAIL}")
    print("=" * 60)

    commits_all: List[CommitFinding] = []

    for repo_full in DEFAULT_REPOS:
        print(f"\n[REPO] Processing repository: {repo_full}")
        try:
            repo = gh.get_repo(repo_full)
            print(f"[REPO] âœ… Successfully connected to {repo_full}")

            commits = repo.get_commits(sha=BRANCH, since=since_utc, until=until_utc)
            commit_list = list(commits)
            print(f"[REPO] ğŸ“Š Found {len(commit_list)} total commits in time range")

            my_commits_count = 0

            for i, c in enumerate(commit_list):
                if not commit_is_mine(c):
                    continue

                # ì»¤ë°‹ ë‹¨ìœ„ ì˜ˆì™¸ ì²˜ë¦¬(ë ˆí¬ ì „ì²´ ì¤‘ë‹¨ ë°©ì§€)
                try:
                    sha = c.sha
                    print(f"[COMMIT] ğŸ” Processing commit {sha[:7]} ({i+1}/{len(commit_list)})")

                    co = repo.get_commit(sha)
                    title = co.commit.message.splitlines()[0].strip()
                    author_name  = co.commit.author.name if co.commit.author else None
                    author_login = co.author.login if co.author else None
                    author_email = co.commit.author.email if co.commit.author else None
                    authored_dt  = co.commit.author.date.replace(tzinfo=tz.UTC).astimezone(tz.gettz("Asia/Seoul"))
                    date_kst_str = authored_dt.strftime("%Y-%m-%d %H:%M:%S %Z")

                    print(f"[COMMIT] ğŸ“ Title: {title}")
                    print(f"[COMMIT] ğŸ‘¤ Author: {author_name} ({author_email})")
                    print(f"[COMMIT] ğŸ“… Date: {date_kst_str}")

                    # íŒŒì¼ë³„ ë¶„ì„ (README ì œì™¸)
                    important_files = [f for f in co.files if 'readme' not in f.filename.lower()]
                    files_to_analyze = important_files
                    print(f"[COMMIT] ğŸ“ Analyzing {len(files_to_analyze)} files (out of {len(co.files)} total) - README excluded")

                    if len(files_to_analyze) == 0:
                        print(f"[COMMIT] â­ï¸  Skipping commit (no files to analyze)")
                        continue

                    file_findings: List[FileFinding] = []
                    for j, f in enumerate(files_to_analyze):
                        print(f"[FILE] ğŸ” Analyzing file {j+1}/{len(files_to_analyze)}: {f.filename} ({f.status})")
                        patch = getattr(f, "patch", None)
                        if not patch:
                            print(f"[FILE] âš ï¸  No patch available for {f.filename} (binary or no diff)")
                            file_findings.append(FileFinding(file_path=f.filename, change_type=f.status, summary="Binary or no diff", risk_level="medium"))
                        else:
                            print(f"[FILE] ğŸ¤– Sending to LLM for analysis...")
                            file_findings.append(analyze_file(f.filename, f.status, patch))
                            print(f"[FILE] âœ… LLM analysis completed for {f.filename}")

                    print(f"[COMMIT] ğŸ¤– Sending commit summary to LLM...")
                    meta = {
                        "repo": repo_full,
                        "sha": sha,
                        "title": title,
                        "author": author_name,
                        "author_login": author_login,
                        "author_email": author_email,
                        "date_kst": date_kst_str
                    }

                    commit_finding = summarize_commit(file_findings, meta)
                    commits_all.append(commit_finding)
                    my_commits_count += 1
                    print(f"[COMMIT] âœ… Completed analysis for {sha[:7]}")

                except BadRequestError as e:
                    msg = getattr(e, "message", str(e))
                    print(f"[COMMIT] âŒ BadRequest on {c.sha[:7]}: {msg}")
                    # íŒŒì¼ ë¶„ì„ì€ ëë‚¬ë‹¤ë©´ Fallbackìœ¼ë¡œë¼ë„ ê¸°ë¡
                    try:
                        commit_finding = fallback_summarize_commit(file_findings, meta)  # type: ignore
                        commits_all.append(commit_finding)
                        my_commits_count += 1
                        print(f"[COMMIT] ğŸ” Fallback summary added for {c.sha[:7]}")
                    except Exception as fe:
                        print(f"[COMMIT] âš ï¸ Fallback failed on {c.sha[:7]}: {fe}")
                    continue
                except Exception as e:
                    print(f"[COMMIT] âŒ Error on {c.sha[:7]}: {e}")
                    # ì›í•˜ë©´ ì—¬ê¸°ì„œë„ fallback ì‹œë„ ê°€ëŠ¥
                    continue

            print(f"[REPO] ğŸ“Š Found {my_commits_count} of your commits in {repo_full}")

        except Exception as e:
            print(f"[REPO] âŒ Error processing {repo_full}: {str(e)}")
            print(f"[REPO] ğŸ”„ Continuing with next repository...")
            continue

    print(f"\n[SUMMARY] ğŸ“Š Analysis completed!")
    print(f"[SUMMARY] ğŸ“ˆ Total commits analyzed: {len(commits_all)}")
    print(f"[SUMMARY] ğŸ“ Generating report...")

    report = ReportModel(
        generated_at=dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        model=OPENAI_MODEL,
        note_date_kst=note_date,
        repos=DEFAULT_REPOS,
        branch=BRANCH,
        author_filter={"login": MY_GITHUB_LOGIN, "email": MY_GITHUB_EMAIL},
        commits=commits_all
    )

    md_path = os.path.join(OUT_DIR, f"research_note_{note_date.replace('-','')}.md")
    json_path = os.path.join(OUT_DIR, f"research_note_{note_date.replace('-','')}.json")

    print(f"[OUTPUT] ğŸ“„ Writing markdown report to: {md_path}")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(render_md(report))

    print(f"[OUTPUT] ğŸ“„ Writing JSON report to: {json_path}")
    with open(json_path, "w", encoding="utf-8") as f:
        f.write(json.dumps(report.to_dict(), indent=2, ensure_ascii=False))

    print(f"\nâœ… ì—°êµ¬ë…¸íŠ¸ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ Markdown: {md_path}")
    print(f"ğŸ“ JSON: {json_path}")
    print(f"ğŸ“Š Total commits: {len(commits_all)}")

if __name__ == "__main__":
    main()
